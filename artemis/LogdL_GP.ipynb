{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cae42c8-2949-4f1b-b061-30eae73bd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import emcee\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import corner\n",
    "import h5py\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "from gpytorch.distributions import MultitaskMultivariateNormal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615c56b-7fee-4951-bcec-a4f7abf13ea6",
   "metadata": {},
   "source": [
    "# Import data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306105c6-d3e5-4216-9df8-004afb7dd774",
   "metadata": {},
   "source": [
    "- will save cosmo parameters as keys in a dict and the values will correspond to logdL in each z bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca216fdd-fe0e-4f3f-b3b9-c75f7bb8bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_tensors(filename):\n",
    "    keys = []\n",
    "    values = []\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        for group_name in hf.keys():\n",
    "            key = hf[group_name]['input_cosmo'][:]\n",
    "            value = hf[group_name]['LogdL'][:]\n",
    "            keys.append(key)\n",
    "            values.append(value)\n",
    "\n",
    "    keys = np.array(keys)\n",
    "    values = np.array(values)\n",
    "\n",
    "    # Convert arrays to PyTorch tensors\n",
    "    data_x = torch.tensor(keys, dtype=torch.float32)  # Assuming keys are numeric\n",
    "    data_y = torch.tensor(values, dtype=torch.float32)\n",
    "    \n",
    "    # Create a dataset from the tensors\n",
    "    dataset = TensorDataset(data_x, data_y)\n",
    "    \n",
    "    # Determine the split sizes\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Unpack the datasets into tensors\n",
    "    train_x = torch.stack([data[0] for data in train_dataset])\n",
    "    train_y = torch.stack([data[1] for data in train_dataset])\n",
    "    test_x = torch.stack([data[0] for data in test_dataset])\n",
    "    test_y = torch.stack([data[1] for data in test_dataset])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "# Load the data and split it\n",
    "train_x, train_y, test_x, test_y = load_data_as_tensors('LogdL_trial.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbb67e07-1950-4eb3-9169-0c7e74380ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3300, 500])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d246c5cb-25f4-4866-8797-5defab098686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultitaskGPModel(ExactGP):\n",
    "#     def __init__(self, train_x, train_y, likelihood):\n",
    "#         super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = ConstantMean()\n",
    "#         self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=train_x.size(-1)))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return MultitaskMultivariateNormal.from_batch_mvn(\n",
    "#             gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "#         )\n",
    "\n",
    "class MultitaskGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=train_x.size(-1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x).unsqueeze(-1).expand(-1, self.likelihood.num_tasks)\n",
    "        covar_x = self.covar_module(x)\n",
    "        task_covar = gpytorch.kernels.IndexKernel(num_tasks=self.likelihood.num_tasks, rank=1)\n",
    "        covar_x = covar_x.add_jitter(1e-4).evaluate()\n",
    "        task_covar = task_covar(torch.arange(self.likelihood.num_tasks)).evaluate()\n",
    "        covar_x = gpytorch.lazy.KroneckerProductLazyTensor(covar_x, task_covar)\n",
    "        \n",
    "        return MultitaskMultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c82480d-1929-45f4-aae3-6bd662d9377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = MultitaskGaussianLikelihood(num_tasks=train_y.size(-1))   # Number of z values, which is 500\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "301516a0-2311-449b-aeee-fbc9f9907e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 - Loss: 1.0838669538497925\n",
      "Iteration 2/50 - Loss: 1.0473957061767578\n",
      "Iteration 3/50 - Loss: 1.0101327896118164\n",
      "Iteration 4/50 - Loss: 0.9720691442489624\n",
      "Iteration 5/50 - Loss: 0.9332306981086731\n",
      "Iteration 6/50 - Loss: 0.8936253190040588\n",
      "Iteration 7/50 - Loss: 0.8532631993293762\n",
      "Iteration 8/50 - Loss: 0.8121418356895447\n",
      "Iteration 9/50 - Loss: 0.7703303694725037\n",
      "Iteration 10/50 - Loss: 0.7277594208717346\n",
      "Iteration 11/50 - Loss: 0.6845454573631287\n",
      "Iteration 12/50 - Loss: 0.6406773328781128\n",
      "Iteration 13/50 - Loss: 0.5961549878120422\n",
      "Iteration 14/50 - Loss: 0.5510791540145874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/distributions/multitask_multivariate_normal.py:212\u001b[0m, in \u001b[0;36mMultitaskMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    210\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    211\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(new_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/distributions/multivariate_normal.py:193\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[1;32m    192\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[0;32m--> 193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/kronecker_product_added_diag_linear_operator.py:73\u001b[0m, in \u001b[0;36mKroneckerProductAddedDiagLinearOperator.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minv_quad_logdet\u001b[39m(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     65\u001b[0m     inv_quad_rhs: Optional[Union[Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N M\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N\u001b[39m\u001b[38;5;124m\"\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     Optional[Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     71\u001b[0m ]:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inv_quad_rhs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m         inv_quad_term, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_inv_quad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_inv_quad\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         inv_quad_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:1712\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inv_quad_rhs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1711\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither `inv_quad_rhs` or `logdet` must be specifed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_inv_quad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_inv_quad\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m   1713\u001b[0m         [], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# Default: use modified batch conjugate gradients to compute these terms\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# See NeurIPS 2018 paper: https://arxiv.org/abs/1809.11165\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_square:\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:1657\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad\u001b[0;34m(self, inv_quad_rhs, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1655\u001b[0m args \u001b[38;5;241m=\u001b[39m (inv_quad_rhs\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39mresult_shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m*\u001b[39minv_quad_rhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]),) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation()\n\u001b[1;32m   1656\u001b[0m func \u001b[38;5;241m=\u001b[39m InvQuad\u001b[38;5;241m.\u001b[39mapply\n\u001b[0;32m-> 1657\u001b[0m inv_quad_term \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_inv_quad:\n\u001b[1;32m   1660\u001b[0m     inv_quad_term \u001b[38;5;241m=\u001b[39m inv_quad_term\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/functions/_inv_quad.py:51\u001b[0m, in \u001b[0;36mInvQuad.forward\u001b[0;34m(ctx, representation_tree, *args)\u001b[0m\n\u001b[1;32m     48\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mis_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m inv_quad_solves \u001b[38;5;241m=\u001b[39m \u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m inv_quad_term \u001b[38;5;241m=\u001b[39m (inv_quad_solves \u001b[38;5;241m*\u001b[39m inv_quad_rhs)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m to_save \u001b[38;5;241m=\u001b[39m matrix_args \u001b[38;5;241m+\u001b[39m [inv_quad_solves]\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/functions/_inv_quad.py:19\u001b[0m, in \u001b[0;36m_solve\u001b[0;34m(linear_op, rhs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     18\u001b[0m     preconditioner \u001b[38;5;241m=\u001b[39m linear_op\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39m_solve_preconditioner()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/kronecker_product_added_diag_linear_operator.py:210\u001b[0m, in \u001b[0;36mKroneckerProductAddedDiagLinearOperator._solve\u001b[0;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInverses of KroneckerProductAddedDiagonals and ConstantDiagLinearOperators are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot implemented yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m         )\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# in this case we can pull across the diagonals\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# (\\otimes K_i + \\otimes D_i) = (\\otimes D_i^{1/2})\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m#   (\\otimes D_i^{-1/2}K_iD_i^{-1/2} + I)(\\otimes D_i^{1/2})\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Reference: Rakitsch, et al, 2013. \"It is all in the noise,\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# https://papers.nips.cc/paper/2013/file/59c33016884a62116be975a9bb8257e3-Paper.pdf\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m dlt_inv_root, evals_p_i, evecs \u001b[38;5;241m=\u001b[39m \u001b[43m_symmetrize_kpadlt_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m res1 \u001b[38;5;241m=\u001b[39m evecs\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\u001b[38;5;241m.\u001b[39mmatmul(dlt_inv_root\u001b[38;5;241m.\u001b[39mmatmul(rhs))\n\u001b[1;32m    213\u001b[0m res2 \u001b[38;5;241m=\u001b[39m evals_p_i\u001b[38;5;241m.\u001b[39msolve(res1)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/kronecker_product_added_diag_linear_operator.py:40\u001b[0m, in \u001b[0;36m_symmetrize_kpadlt_constructor\u001b[0;34m(lt, dlt)\u001b[0m\n\u001b[1;32m     36\u001b[0m dlt_inv_root \u001b[38;5;241m=\u001b[39m dlt\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39minverse()\n\u001b[1;32m     37\u001b[0m symm_prod \u001b[38;5;241m=\u001b[39m KroneckerProductLinearOperator(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m*\u001b[39m[d\u001b[38;5;241m.\u001b[39mmatmul(k)\u001b[38;5;241m.\u001b[39mmatmul(d) \u001b[38;5;28;01mfor\u001b[39;00m k, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lt\u001b[38;5;241m.\u001b[39mlinear_ops, dlt_inv_root\u001b[38;5;241m.\u001b[39mlinear_ops)]\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m evals, evecs \u001b[38;5;241m=\u001b[39m \u001b[43msymm_prod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagonalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m evals_plus_i \u001b[38;5;241m=\u001b[39m DiagLinearOperator(evals \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dlt_inv_root, evals_plus_i, evecs\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/kronecker_product_linear_operator.py:148\u001b[0m, in \u001b[0;36mKroneckerProductLinearOperator.diagonalization\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymeig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagonalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:1453\u001b[0m, in \u001b[0;36mLinearOperator.diagonalization\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     evecs \u001b[38;5;241m=\u001b[39m to_linear_operator(evecs)\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymeig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1453\u001b[0m     evals, evecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_symeig\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown diagonalization method \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/kronecker_product_linear_operator.py:338\u001b[0m, in \u001b[0;36mKroneckerProductLinearOperator._symeig\u001b[0;34m(self, eigenvectors, return_evals_as_lazy)\u001b[0m\n\u001b[1;32m    336\u001b[0m evals, evecs \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops:\n\u001b[0;32m--> 338\u001b[0m     evals_, evecs_ \u001b[38;5;241m=\u001b[39m \u001b[43mlt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_symeig\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meigenvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     evals\u001b[38;5;241m.\u001b[39mappend(evals_)\n\u001b[1;32m    340\u001b[0m     evecs\u001b[38;5;241m.\u001b[39mappend(evecs_)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:886\u001b[0m, in \u001b[0;36mLinearOperator._symeig\u001b[0;34m(self, eigenvectors, return_evals_as_lazy)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# potentially perform decomposition in double precision for numerical stability\u001b[39;00m\n\u001b[1;32m    885\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 886\u001b[0m evals, evecs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linalg_dtype_symeig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# chop any negative eigenvalues.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# TODO: warn if evals are significantly negative\u001b[39;00m\n\u001b[1;32m    889\u001b[0m evals \u001b[38;5;241m=\u001b[39m evals\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "num_iterations = 50\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Iteration {i + 1}/{num_iterations} - Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2214dedc-172f-4041-8c80-577428640dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 10890000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m likelihood\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), gpytorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mfast_pred_var():\n\u001b[0;32m----> 5\u001b[0m     observed_pred \u001b[38;5;241m=\u001b[39m likelihood(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# The output is a MultitaskMultivariateNormal. To get the mean predictions:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m mean_predictions \u001b[38;5;241m=\u001b[39m observed_pred\u001b[38;5;241m.\u001b[39mmean\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:290\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    285\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    286\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_predictive_covar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_test_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_train_covar\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:356\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# In other cases - we'll use the standard infrastructure\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m test_test_covar \u001b[38;5;241m+\u001b[39m MatmulLinearOperator(test_train_covar, covar_correction_rhs\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 356\u001b[0m precomputed_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_cache\u001b[49m\n\u001b[1;32m    357\u001b[0m covar_inv_quad_form_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exact_predictive_covar_inv_quad_form_root(precomputed_cache, test_train_covar)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(test_test_covar):\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:246\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.covar_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;129m@cached\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovar_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcovar_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    245\u001b[0m     train_train_covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlik_train_train_covar\n\u001b[0;32m--> 246\u001b[0m     train_train_covar_inv_root \u001b[38;5;241m=\u001b[39m \u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_train_covar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_inv_decomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_test_train_covar)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2985\u001b[0m, in \u001b[0;36mto_dense\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, LinearOperator):\n\u001b[0;32m-> 2985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject of class \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be made into a Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/matmul_linear_operator.py:132\u001b[0m, in \u001b[0;36mMatmulLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_linear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_linear_op\u001b[38;5;241m.\u001b[39mto_dense())\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/diag_linear_operator.py:137\u001b[0m, in \u001b[0;36mDiagLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diag\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 10890000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "# The output is a MultitaskMultivariateNormal. To get the mean predictions:\n",
    "mean_predictions = observed_pred.mean\n",
    "\n",
    "# mean_predictions will be of shape (num_samples, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb403a7-22cd-4ce7-8de9-1732fd634412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ssi2023)",
   "language": "python",
   "name": "ssi_torchvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
