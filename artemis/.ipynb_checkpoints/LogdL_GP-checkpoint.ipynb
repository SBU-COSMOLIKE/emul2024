{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cae42c8-2949-4f1b-b061-30eae73bd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import emcee\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import corner\n",
    "import h5py\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import MultitaskGaussianLikelihood\n",
    "from gpytorch.distributions import MultitaskMultivariateNormal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615c56b-7fee-4951-bcec-a4f7abf13ea6",
   "metadata": {},
   "source": [
    "# Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca216fdd-fe0e-4f3f-b3b9-c75f7bb8bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_tensors(filename):\n",
    "    keys = []\n",
    "    values = []\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        for group_name in hf.keys():\n",
    "            key = hf[group_name]['input_cosmo'][:]\n",
    "            value = hf[group_name]['LogdL'][:]\n",
    "            keys.append(key)\n",
    "            values.append(value)\n",
    "\n",
    "    keys = np.array(keys)\n",
    "    values = np.array(values)\n",
    "\n",
    "    # Convert arrays to PyTorch tensors\n",
    "    data_x = torch.tensor(keys, dtype=torch.float32)  # Assuming keys are numeric\n",
    "    data_y = torch.tensor(values, dtype=torch.float32)\n",
    "    \n",
    "    # Create a dataset from the tensors\n",
    "    dataset = TensorDataset(data_x, data_y)\n",
    "    \n",
    "    # Determine the split sizes\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Unpack the datasets into tensors\n",
    "    train_x = torch.stack([data[0] for data in train_dataset])\n",
    "    train_y = torch.stack([data[1] for data in train_dataset])\n",
    "    test_x = torch.stack([data[0] for data in test_dataset])\n",
    "    test_y = torch.stack([data[1] for data in test_dataset])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "# Load the data and split it\n",
    "train_x, train_y, test_x, test_y = load_data_as_tensors('LogdL_trial.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbb67e07-1950-4eb3-9169-0c7e74380ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3300, 500])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d246c5cb-25f4-4866-8797-5defab098686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultitaskGPModel(ExactGP):\n",
    "#     def __init__(self, train_x, train_y, likelihood):\n",
    "#         super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = ConstantMean()\n",
    "#         self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=train_x.size(-1)))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return MultitaskMultivariateNormal.from_batch_mvn(\n",
    "#             gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "#         )\n",
    "\n",
    "class MultitaskGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=train_x.size(-1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x).unsqueeze(-1).expand(-1, self.likelihood.num_tasks)\n",
    "        covar_x = self.covar_module(x)\n",
    "        task_covar = gpytorch.kernels.IndexKernel(num_tasks=self.likelihood.num_tasks, rank=1)\n",
    "        covar_x = covar_x.add_jitter(1e-4).evaluate()\n",
    "        task_covar = task_covar(torch.arange(self.likelihood.num_tasks)).evaluate()\n",
    "        covar_x = gpytorch.lazy.KroneckerProductLazyTensor(covar_x, task_covar)\n",
    "        \n",
    "        return MultitaskMultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c82480d-1929-45f4-aae3-6bd662d9377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = MultitaskGaussianLikelihood(num_tasks=train_y.size(-1))   # Number of z values, which is 500\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301516a0-2311-449b-aeee-fbc9f9907e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 - Loss: 1.0838841199874878\n",
      "Iteration 2/50 - Loss: 1.0473910570144653\n",
      "Iteration 3/50 - Loss: 1.0101312398910522\n",
      "Iteration 4/50 - Loss: 0.9720796346664429\n",
      "Iteration 5/50 - Loss: 0.9332265257835388\n",
      "Iteration 6/50 - Loss: 0.8936579823493958\n",
      "Iteration 7/50 - Loss: 0.8532719612121582\n",
      "Iteration 8/50 - Loss: 0.8121502995491028\n",
      "Iteration 9/50 - Loss: 0.770314633846283\n",
      "Iteration 10/50 - Loss: 0.7277572751045227\n",
      "Iteration 11/50 - Loss: 0.684590756893158\n",
      "Iteration 12/50 - Loss: 0.6406735777854919\n",
      "Iteration 13/50 - Loss: 0.5961640477180481\n",
      "Iteration 14/50 - Loss: 0.5510638356208801\n",
      "Iteration 15/50 - Loss: 0.5053346753120422\n",
      "Iteration 16/50 - Loss: 0.4590665400028229\n",
      "Iteration 17/50 - Loss: 0.41231849789619446\n",
      "Iteration 18/50 - Loss: 0.36502665281295776\n",
      "Iteration 19/50 - Loss: 0.3172742426395416\n",
      "Iteration 20/50 - Loss: 0.2690575122833252\n",
      "Iteration 21/50 - Loss: 0.22048652172088623\n",
      "Iteration 22/50 - Loss: 0.17146076261997223\n",
      "Iteration 23/50 - Loss: 0.1221318170428276\n",
      "Iteration 24/50 - Loss: 0.07248643785715103\n",
      "Iteration 25/50 - Loss: 0.02243378758430481\n",
      "Iteration 26/50 - Loss: -0.027800757437944412\n",
      "Iteration 27/50 - Loss: -0.07831750065088272\n",
      "Iteration 28/50 - Loss: -0.1290091723203659\n",
      "Iteration 29/50 - Loss: -0.18006621301174164\n",
      "Iteration 30/50 - Loss: -0.23113644123077393\n",
      "Iteration 31/50 - Loss: -0.28247544169425964\n",
      "Iteration 32/50 - Loss: -0.3339584767818451\n",
      "Iteration 33/50 - Loss: -0.38554492592811584\n",
      "Iteration 34/50 - Loss: -0.43723204731941223\n",
      "Iteration 35/50 - Loss: -0.4890214502811432\n",
      "Iteration 36/50 - Loss: -0.5410012602806091\n",
      "Iteration 37/50 - Loss: -0.5928840041160583\n",
      "Iteration 38/50 - Loss: -0.6448805332183838\n",
      "Iteration 39/50 - Loss: -0.6968464255332947\n",
      "Iteration 40/50 - Loss: -0.748853862285614\n",
      "Iteration 41/50 - Loss: -0.8007810115814209\n",
      "Iteration 42/50 - Loss: -0.8529163002967834\n",
      "Iteration 43/50 - Loss: -0.9048161506652832\n",
      "Iteration 44/50 - Loss: -0.9567264318466187\n",
      "Iteration 45/50 - Loss: -1.0085784196853638\n",
      "Iteration 46/50 - Loss: -1.0603001117706299\n",
      "Iteration 47/50 - Loss: -1.111942172050476\n",
      "Iteration 48/50 - Loss: -1.1637017726898193\n",
      "Iteration 49/50 - Loss: -1.2149626016616821\n",
      "Iteration 50/50 - Loss: -1.2662806510925293\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "num_iterations = 50\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Iteration {i + 1}/{num_iterations} - Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4757a687-a765-4c1d-8f9a-e54a3c35aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model \n",
    "torch.save(model.state_dict(), 'model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2214dedc-172f-4041-8c80-577428640dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## When I execute this, it overflows my memory. The dense covariance matrix that I calculated in the training step \n",
    "# model.eval()\n",
    "# likelihood.eval()\n",
    "\n",
    "# with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#     observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "# # The output is a MultitaskMultivariateNormal. To get the mean predictions:\n",
    "# mean_predictions = observed_pred.mean\n",
    "\n",
    "# # mean_predictions will be of shape (num_samples, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d390dd-2436-4381-a1b6-27d272600c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('model_state.pth')\n",
    "model = MultitaskGPModel(train_x, train_y, likelihood)  # Create a new GP model\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b35bda5-c026-4daf-933d-81575fa43be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.raw_task_noises',\n",
       "              tensor([-5.1974, -5.1967, -5.1973, -5.1970, -5.1976, -5.1972, -5.1971, -5.1974,\n",
       "                      -5.1971, -5.1975, -5.1979, -5.1971, -5.1972, -5.1975, -5.1976, -5.1967,\n",
       "                      -5.1975, -5.1971, -5.1972, -5.1973, -5.1967, -5.1975, -5.1973, -5.1966,\n",
       "                      -5.1966, -5.1968, -5.1976, -5.1973, -5.1972, -5.1970, -5.1976, -5.1972,\n",
       "                      -5.1974, -5.1975, -5.1974, -5.1975, -5.1978, -5.1972, -5.1980, -5.1971,\n",
       "                      -5.1974, -5.1975, -5.1978, -5.1977, -5.1980, -5.1976, -5.1978, -5.1974,\n",
       "                      -5.1973, -5.1971, -5.1965, -5.1969, -5.1973, -5.1972, -5.1973, -5.1975,\n",
       "                      -5.1976, -5.1971, -5.1974, -5.1974, -5.1974, -5.1972, -5.1971, -5.1972,\n",
       "                      -5.1972, -5.1974, -5.1973, -5.1965, -5.1975, -5.1974, -5.1979, -5.1981,\n",
       "                      -5.1975, -5.1973, -5.1969, -5.1980, -5.1981, -5.1970, -5.1974, -5.1970,\n",
       "                      -5.1972, -5.1975, -5.1976, -5.1972, -5.1975, -5.1974, -5.1975, -5.1977,\n",
       "                      -5.1975, -5.1976, -5.1980, -5.1970, -5.1970, -5.1976, -5.1972, -5.1975,\n",
       "                      -5.1967, -5.1974, -5.1972, -5.1973, -5.1973, -5.1970, -5.1978, -5.1974,\n",
       "                      -5.1970, -5.1972, -5.1976, -5.1974, -5.1979, -5.1974, -5.1980, -5.1973,\n",
       "                      -5.1973, -5.1971, -5.1973, -5.1974, -5.1973, -5.1978, -5.1974, -5.1963,\n",
       "                      -5.1969, -5.1975, -5.1971, -5.1973, -5.1965, -5.1968, -5.1978, -5.1966,\n",
       "                      -5.1973, -5.1972, -5.1970, -5.1973, -5.1979, -5.1975, -5.1980, -5.1971,\n",
       "                      -5.1978, -5.1973, -5.1981, -5.1975, -5.1978, -5.1969, -5.1971, -5.1975,\n",
       "                      -5.1977, -5.1975, -5.1970, -5.1972, -5.1972, -5.1978, -5.1969, -5.1977,\n",
       "                      -5.1977, -5.1968, -5.1971, -5.1973, -5.1976, -5.1975, -5.1979, -5.1971,\n",
       "                      -5.1977, -5.1975, -5.1970, -5.1974, -5.1974, -5.1974, -5.1977, -5.1972,\n",
       "                      -5.1977, -5.1978, -5.1978, -5.1970, -5.1970, -5.1966, -5.1973, -5.1967,\n",
       "                      -5.1980, -5.1971, -5.1975, -5.1972, -5.1967, -5.1974, -5.1978, -5.1979,\n",
       "                      -5.1977, -5.1966, -5.1966, -5.1972, -5.1976, -5.1978, -5.1973, -5.1970,\n",
       "                      -5.1974, -5.1975, -5.1977, -5.1972, -5.1974, -5.1976, -5.1976, -5.1969,\n",
       "                      -5.1975, -5.1972, -5.1979, -5.1970, -5.1971, -5.1968, -5.1970, -5.1970,\n",
       "                      -5.1981, -5.1970, -5.1965, -5.1977, -5.1974, -5.1967, -5.1975, -5.1969,\n",
       "                      -5.1970, -5.1979, -5.1963, -5.1977, -5.1968, -5.1975, -5.1971, -5.1978,\n",
       "                      -5.1975, -5.1970, -5.1969, -5.1971, -5.1970, -5.1980, -5.1976, -5.1978,\n",
       "                      -5.1972, -5.1976, -5.1968, -5.1975, -5.1975, -5.1977, -5.1969, -5.1970,\n",
       "                      -5.1969, -5.1974, -5.1972, -5.1973, -5.1965, -5.1976, -5.1973, -5.1972,\n",
       "                      -5.1976, -5.1975, -5.1969, -5.1971, -5.1975, -5.1969, -5.1973, -5.1970,\n",
       "                      -5.1968, -5.1977, -5.1972, -5.1967, -5.1967, -5.1973, -5.1980, -5.1968,\n",
       "                      -5.1974, -5.1973, -5.1974, -5.1970, -5.1971, -5.1970, -5.1976, -5.1975,\n",
       "                      -5.1975, -5.1972, -5.1979, -5.1967, -5.1968, -5.1982, -5.1973, -5.1971,\n",
       "                      -5.1975, -5.1974, -5.1973, -5.1973, -5.1975, -5.1971, -5.1973, -5.1976,\n",
       "                      -5.1980, -5.1974, -5.1970, -5.1974, -5.1963, -5.1971, -5.1971, -5.1973,\n",
       "                      -5.1965, -5.1974, -5.1974, -5.1975, -5.1976, -5.1973, -5.1971, -5.1970,\n",
       "                      -5.1968, -5.1972, -5.1973, -5.1972, -5.1969, -5.1974, -5.1971, -5.1977,\n",
       "                      -5.1971, -5.1970, -5.1974, -5.1975, -5.1972, -5.1974, -5.1972, -5.1971,\n",
       "                      -5.1973, -5.1972, -5.1976, -5.1966, -5.1977, -5.1972, -5.1978, -5.1971,\n",
       "                      -5.1971, -5.1969, -5.1971, -5.1972, -5.1964, -5.1969, -5.1977, -5.1975,\n",
       "                      -5.1970, -5.1972, -5.1975, -5.1971, -5.1971, -5.1973, -5.1975, -5.1979,\n",
       "                      -5.1974, -5.1975, -5.1974, -5.1967, -5.1968, -5.1970, -5.1965, -5.1966,\n",
       "                      -5.1974, -5.1979, -5.1978, -5.1974, -5.1968, -5.1969, -5.1971, -5.1973,\n",
       "                      -5.1974, -5.1977, -5.1970, -5.1966, -5.1976, -5.1973, -5.1975, -5.1979,\n",
       "                      -5.1973, -5.1977, -5.1967, -5.1972, -5.1976, -5.1974, -5.1969, -5.1965,\n",
       "                      -5.1966, -5.1976, -5.1975, -5.1972, -5.1967, -5.1976, -5.1974, -5.1977,\n",
       "                      -5.1974, -5.1972, -5.1978, -5.1968, -5.1978, -5.1970, -5.1973, -5.1972,\n",
       "                      -5.1980, -5.1976, -5.1974, -5.1975, -5.1969, -5.1970, -5.1974, -5.1969,\n",
       "                      -5.1964, -5.1979, -5.1976, -5.1967, -5.1970, -5.1971, -5.1966, -5.1975,\n",
       "                      -5.1967, -5.1971, -5.1973, -5.1972, -5.1972, -5.1971, -5.1971, -5.1972,\n",
       "                      -5.1969, -5.1972, -5.1974, -5.1963, -5.1972, -5.1970, -5.1967, -5.1968,\n",
       "                      -5.1973, -5.1975, -5.1972, -5.1970, -5.1973, -5.1973, -5.1967, -5.1976,\n",
       "                      -5.1975, -5.1972, -5.1972, -5.1973, -5.1972, -5.1969, -5.1973, -5.1977,\n",
       "                      -5.1976, -5.1965, -5.1973, -5.1973, -5.1972, -5.1973, -5.1971, -5.1974,\n",
       "                      -5.1971, -5.1974, -5.1978, -5.1973, -5.1973, -5.1975, -5.1975, -5.1980,\n",
       "                      -5.1976, -5.1975, -5.1971, -5.1975, -5.1969, -5.1972, -5.1980, -5.1972,\n",
       "                      -5.1980, -5.1969, -5.1968, -5.1970, -5.1967, -5.1972, -5.1977, -5.1971,\n",
       "                      -5.1975, -5.1977, -5.1967, -5.1975, -5.1973, -5.1973, -5.1967, -5.1963,\n",
       "                      -5.1966, -5.1972, -5.1973, -5.1968, -5.1970, -5.1971, -5.1972, -5.1973,\n",
       "                      -5.1973, -5.1973, -5.1976, -5.1971, -5.1977, -5.1971, -5.1972, -5.1974,\n",
       "                      -5.1973, -5.1971, -5.1974, -5.1973])),\n",
       "             ('likelihood.raw_noise', tensor([-5.1974])),\n",
       "             ('likelihood.raw_task_noises_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.raw_task_noises_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('likelihood.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.raw_noise_constraint.upper_bound', tensor(inf)),\n",
       "             ('mean_module.raw_constant', tensor(0.1022)),\n",
       "             ('covar_module.raw_outputscale', tensor(-1.2319)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale',\n",
       "              tensor([[4.0342, 4.0358, 0.1971, 3.6516]])),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_lengthscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a5739f-5392-42ca-a962-9f9ce2358768",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 10890000000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m test_batch \u001b[38;5;241m=\u001b[39m test_x[batch_start:batch_end]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Make predictions for the current batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m observed_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m mean_predictions_batch \u001b[38;5;241m=\u001b[39m likelihood(observed_pred)\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m     20\u001b[0m mean_predictions\u001b[38;5;241m.\u001b[39mappend(mean_predictions_batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:290\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    285\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    286\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_predictive_covar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_test_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_train_covar\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:356\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# In other cases - we'll use the standard infrastructure\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m test_test_covar \u001b[38;5;241m+\u001b[39m MatmulLinearOperator(test_train_covar, covar_correction_rhs\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 356\u001b[0m precomputed_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovar_cache\u001b[49m\n\u001b[1;32m    357\u001b[0m covar_inv_quad_form_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exact_predictive_covar_inv_quad_form_root(precomputed_cache, test_train_covar)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(test_test_covar):\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:246\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.covar_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;129m@cached\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovar_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcovar_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    245\u001b[0m     train_train_covar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlik_train_train_covar\n\u001b[0;32m--> 246\u001b[0m     train_train_covar_inv_root \u001b[38;5;241m=\u001b[39m \u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_train_covar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_inv_decomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_test_train_covar)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2985\u001b[0m, in \u001b[0;36mto_dense\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, LinearOperator):\n\u001b[0;32m-> 2985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject of class \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cannot be made into a Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/matmul_linear_operator.py:132\u001b[0m, in \u001b[0;36mMatmulLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_linear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_linear_op\u001b[38;5;241m.\u001b[39mto_dense())\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/ssi2023/lib/python3.11/site-packages/linear_operator/operators/diag_linear_operator.py:137\u001b[0m, in \u001b[0;36mDiagLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diag\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 10890000000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    # Generate predictions in batches to avoid large memory usage\n",
    "    batch_size = 1  # Adjust this based on your available memory\n",
    "    num_batches = int(np.ceil(len(test_x) / batch_size))\n",
    "    \n",
    "    mean_predictions = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = min((i + 1) * batch_size, len(test_x))\n",
    "        test_batch = test_x[batch_start:batch_end]\n",
    "        \n",
    "        # Make predictions for the current batch\n",
    "        observed_pred = model(test_batch)\n",
    "        mean_predictions_batch = likelihood(observed_pred).mean\n",
    "        \n",
    "        mean_predictions.append(mean_predictions_batch)\n",
    "    \n",
    "    # Concatenate all batch predictions\n",
    "    mean_predictions = torch.cat(mean_predictions, dim=0)\n",
    "\n",
    "# mean_predictions will be of shape (num_samples, 500)\n",
    "print(mean_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb403a7-22cd-4ce7-8de9-1732fd634412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ssi2023)",
   "language": "python",
   "name": "ssi_torchvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
